{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c370c1f0-eca7-4543-8fce-1a77d8e48572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-04-23 12:45:05 nemo_logging:349] /home/joheras/.local/lib/python3.10/site-packages/hydra/core/plugins.py:225: UserWarning: \n",
      "    \tError importing 'hydra_plugins.hydra_colorlog'.\n",
      "    \tPlugin is incompatible with this Hydra version or buggy.\n",
      "    \tRecommended to uninstall or upgrade plugin.\n",
      "    \t\tImportError : cannot import name 'SearchPathPlugin' from 'hydra.plugins' (/home/joheras/.local/lib/python3.10/site-packages/hydra/plugins/__init__.py)\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 12:45:07.087721: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[NeMo W 2024-04-23 12:45:08 nemo_logging:349] /home/joheras/.conda/envs/hf/lib/python3.10/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-04-23 12:45:08 exp_manager:394] Experiments will be logged at SelfAlignmentPretrainingTinyExample/2024-04-23_12-45-08\n",
      "[NeMo I 2024-04-23 12:45:08 exp_manager:835] TensorboardLogger has been set up\n",
      "[NeMo I 2024-04-23 12:45:09 entity_linking_dataset:78] Loaded dataset with 171466 examples\n",
      "[NeMo I 2024-04-23 12:45:09 entity_linking_dataset:78] Loaded dataset with 1000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-04-23 12:45:09 nlp_overrides:457] Apex was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/apex\n",
      "    Megatron-based models require Apex to function correctly.\n",
      "[NeMo W 2024-04-23 12:45:09 lm_utils:91] bert-base-multilingual-uncased is not in get_pretrained_lm_models_list(include_external=False), will be using AutoModel from HuggingFace.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Lightning can't create new processes if CUDA is already initialized. Did you manually call `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any other way? Please remove any such calls, or change the selected strategy. You will have to restart the Python kernel.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m model \u001b[38;5;241m=\u001b[39m nemo_nlp\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mEntityLinkingModel(cfg\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mmodel, trainer\u001b[38;5;241m=\u001b[39mtrainer)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# In[6]:\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Train and save the model\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m model\u001b[38;5;241m.\u001b[39msave_to(cfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnemo_path)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HfApi\n",
      "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:520\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    518\u001b[0m model \u001b[38;5;241m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 520\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:42\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:99\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_torchdistx_support()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_method \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfork\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforkserver\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 99\u001b[0m     \u001b[43m_check_bad_cuda_fork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# The default cluster environment in Lightning chooses a random free port number\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# This needs to be done in the main process here before starting processes to ensure each rank will connect\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# through the same port\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy\u001b[38;5;241m.\u001b[39mcluster_environment \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/lightning_fabric/strategies/launchers/multiprocessing.py:189\u001b[0m, in \u001b[0;36m_check_bad_cuda_fork\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_INTERACTIVE:\n\u001b[1;32m    188\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m You will have to restart the Python kernel.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(message)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Lightning can't create new processes if CUDA is already initialized. Did you manually call `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any other way? Please remove any such calls, or change the selected strategy. You will have to restart the Python kernel."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import faiss\n",
    "import torch\n",
    "import wget\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from pytorch_lightning import Trainer\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# # Download data into project directory\n",
    "PROJECT_DIR = \".\" #Change if you don't want the current directory to be the project dir\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, \"DataLeo\")\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# Download config\n",
    "# wget.download(\"https://raw.githubusercontent.com/vadam5/NeMo/main/examples/nlp/entity_linking/conf/tiny_example_entity_linking_config.yaml\",\n",
    "#               os.path.join(PROJECT_DIR, \"tiny_example_entity_linking_config.yaml\"))\n",
    "\n",
    "# Load in config file\n",
    "cfg = OmegaConf.load(os.path.join(PROJECT_DIR, \"tiny_example_entity_linking_config (1).yaml\"))\n",
    "\n",
    "# Set config file variables\n",
    "cfg.project_dir = PROJECT_DIR\n",
    "cfg.model.nemo_path = os.path.join(PROJECT_DIR, \"sap_bert_model.nemo\")\n",
    "cfg.model.train_ds.data_file = os.path.join(DATA_DIR, \"train.txt\")\n",
    "cfg.model.validation_ds.data_file = os.path.join(DATA_DIR, \"val.txt\")\n",
    "\n",
    "# Initialize the trainer and model\n",
    "trainer = Trainer(**cfg.trainer)\n",
    "exp_manager(trainer, cfg.get(\"exp_manager\", None))\n",
    "model = nemo_nlp.models.EntityLinkingModel(cfg=cfg.model, trainer=trainer)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "# Train and save the model\n",
    "trainer.fit(model)\n",
    "model.save_to(cfg.model.nemo_path)\n",
    "\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "username = api.whoami()['name']\n",
    "\n",
    "# In[ ]:\n",
    "MODEL_NAME = \"SelfAlignmentPretrainingForMedicalEntityLinking-ClaraMeD-FineTuned\"\n",
    "\n",
    "try:\n",
    "  api.create_repo(repo_id=MODEL_NAME)\n",
    "  print(\"Successfully created repository !\")\n",
    "except Exception as e:\n",
    "  print(\"Repository is possibly already created. Refer to error here - \\n\\n\", e)\n",
    "\n",
    "from huggingface_hub import Repository\n",
    "local_dir = f'model-{MODEL_NAME}/'\n",
    "hf_model_name = f'{username}/{MODEL_NAME}'\n",
    "\n",
    "commit_message = \"Upload model\"\n",
    "model_filename = f'{MODEL_NAME}.nemo'\n",
    "\n",
    "with Repository(local_dir=local_dir, clone_from=hf_model_name, repo_type='model').commit(commit_message):\n",
    "  model.save_to(model_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe433f7-f20b-430d-8b1c-b24c76937ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-hf]",
   "language": "python",
   "name": "conda-env-.conda-hf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
